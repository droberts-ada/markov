Creating an NPM package:
https://medium.freecodecamp.org/how-to-make-a-beautiful-tiny-npm-package-and-publish-it-2881d4307f78

Reading files:
https://nodejs.org/api/fs.html#fs_file_system

Jest for testing
https://jestjs.io/docs/en/getting-started

Debug for logging
DEBUG=markov bin/main.js

TODO List 5/10:
- [x] Handle punctuation (period, comma, citation, etc)
  - Tokenization
  - Artifact generation
- [ ] Figure out how to support parens and quotes
  - Need to keep track of more state ("are we in a quote right now?").
  - Way to keep things relatively short? Maybe each token has a "probability to end a quote" and "probability to end a parenthetical", which is paid attention to more over time?
    - Could do this for sentences and paragraphs too, to keep lengths realistic.
    - Need to think about nesting (should finish quote before parens)
- [ ] Capitalization (input and output)
  - [x] First word of sentence
  - [ ] Proper nouns
- [ ] Finish with a period
  - Limit is soft, find the last period and go until the next period, figure out which gets you closer to the goal
  - Increase the probability of a period around the word count.
  - Possibly related to work on parens and quotes above.
- Multi-token history (input and output)
  - [x] Fixed length of `h`
  - [ ] How to assess quality of generated text, given different histories?
  - [ ] Track all lengths up to max length, and somehow use this to generate (think about measure of how idiomatic a word is, how narrow is its use?)
    - Shouldn't take too much extra space - (max) number of states is `v^h`, where `v` is vocab size and `h` is history length, so adding in `v^h-1 ... v^1` should effectively not matter.
  - [ ] Do something special for things like punctuation (maybe not needed)
- [ ] Get more / better test data
  - More random texts for fun
  - List of scientific paper titles or conference talks (where to find?)

Advanced analysis / NLP
- [ ] Topics for paragraphs
  - Bag of words / k-means to cluster paragraphs, then figure out the most "distinctive" word for each. Maybe transitions are weighted based on topic, particularly for more distinctive words?
  - Fuck HTF do you test this?

Vocab list
- Source document / document: one thing to read
- Text: a string representing the un-processed words of a source document
- Token: the generator's representation of a bit of text. Could be a literal (the word itself) or an escape
- Tokenize: turn a text into a list of tokens
- Escape: a non-literal token. In angle brackets, representing things like punctuation, new paragraphs, or wikipedia citation markers.
- Transition: each token gets a set of transitions, representing the probability of hopping to other possible tokens
- Target: a possible token that you might hop to. Coupled with a probability it makes a transition
- Artifact: a string generated by this program
- Assemble: turn a list of tokens into an artifact (lots of de-escaping, mucking with whitespace, etc)
- Flow is:
    ```
                 tokenize          process        generate          assemble
    source document -> list of tokens -> transitions -> list of tokens -> artifact
    ```